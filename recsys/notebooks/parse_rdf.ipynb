{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import urlparse\n",
      "import urllib\n",
      "\n",
      "def fixurl(url):\n",
      "    # turn string into unicode\n",
      "    if not isinstance(url,unicode):\n",
      "        url = url.decode('utf8')\n",
      "\n",
      "    # parse it\n",
      "    parsed = urlparse.urlsplit(url)\n",
      "\n",
      "    # divide the netloc further\n",
      "    userpass,at,hostport = parsed.netloc.rpartition('@')\n",
      "    user,colon1,pass_ = userpass.partition(':')\n",
      "    host,colon2,port = hostport.partition(':')\n",
      "\n",
      "    # encode each component\n",
      "    scheme = parsed.scheme.encode('utf8')\n",
      "    user = urllib.quote(user.encode('utf8'))\n",
      "    colon1 = colon1.encode('utf8')\n",
      "    pass_ = urllib.quote(pass_.encode('utf8'))\n",
      "    at = at.encode('utf8')\n",
      "    host = host.encode('idna')\n",
      "    colon2 = colon2.encode('utf8')\n",
      "    port = port.encode('utf8')\n",
      "    path = '/'.join(  # could be encoded slashes!\n",
      "        urllib.quote(urllib.unquote(pce).encode('utf8'),'')\n",
      "        for pce in parsed.path.split('/')\n",
      "    )\n",
      "    query = urllib.quote(urllib.unquote(parsed.query).encode('utf8'),'=&?/')\n",
      "    fragment = urllib.quote(urllib.unquote(parsed.fragment).encode('utf8'))\n",
      "\n",
      "    # put it back together\n",
      "    netloc = ''.join((user,colon1,pass_,at,host,colon2,port))\n",
      "    return urlparse.urlunsplit((scheme,netloc,path,query,fragment))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 170
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from langdetect import detect\n",
      "from rdflib import URIRef\n",
      "from time import sleep\n",
      "import urllib2\n",
      "\n",
      "filename = '/home/behrouz/joey/Docs/Personal/ml_challenge/recsys/data/items_books.dat'\n",
      "problematic = [u'http://dbpedia.org/resource/The_Adventures_of_Tintin']\n",
      "\n",
      "books = []\n",
      "with open(filename, 'r') as f:\n",
      "    f.readline()\n",
      "    for line in f:\n",
      "        id_, type_ , entity_ = line.strip().split()\n",
      "        if type_ == 'book':\n",
      "            title = entity_.decode('latin-1')\n",
      "            if not fixurl(title) in problematic:\n",
      "                books.append((id_, title))\n",
      "            \n",
      "def get_info(book):\n",
      "    id = book[0]\n",
      "    url = book[1]\n",
      "    g = rdflib.Graph()\n",
      "    print fixurl(url)\n",
      "    done = False\n",
      "    while not done:\n",
      "        try:\n",
      "            result = g.parse(fixurl(url))\n",
      "        except urllib2.HTTPError:\n",
      "            sleep(0.5)\n",
      "        else:\n",
      "            authors = [unicode(author) for author in g.objects(URIRef(url), URIRef(u'http://dbpedia.org/property/author'))]\n",
      "            genres = [unicode(genre) for genre in g.objects(URIRef(url), URIRef(u'http://dbpedia.org/property/genre'))]\n",
      "            abstracts = [abstract for abstract in g.objects(URIRef(url), URIRef(u'http://dbpedia.org/ontology/abstract'))\n",
      "                         if len(abstract) > 10 and detect(unicode(abstract))=='en']\n",
      "            abstract = None\n",
      "            if len(abstracts) > 0:\n",
      "                abstract = abstracts[0]\n",
      "            done = True\n",
      "\n",
      "    return (authors, genres, abstract)\n",
      "\n",
      "book_info = dict()\n",
      "all_info = []\n",
      "for book in books:\n",
      "    info = get_info(book)\n",
      "    book_info['id'] = book[0]\n",
      "    book_info['url'] = book[1]\n",
      "    book_info['author'] = info[0]\n",
      "    book_info['genres'] = info[1]\n",
      "    book_info['abstract'] = info[2]\n",
      "    all_info.append(book_info)\n",
      "pickle.dump(all_info, open('all.pkl', 'w'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 196
    }
   ],
   "metadata": {}
  }
 ]
}